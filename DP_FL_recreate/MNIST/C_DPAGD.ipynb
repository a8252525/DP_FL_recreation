{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import random\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = 20\n",
    "        self.best_lr_list = []\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 10\n",
    "        self.save_model = False\n",
    "        self.gamma = 0.1\n",
    "        self.alpha_max = 0.1\n",
    "        self.epsilon = 8\n",
    "        self.clip_threshold = 0.01\n",
    "        self.split = 600\n",
    "        \n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory = True\n",
    "\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size,\n",
    "    pin_memory = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 34, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(34, 64, 5, 1)\n",
    "        self.fc1 = nn.Linear(20*20*64, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 20*20*64)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# model is not exactully the same as the paper since it did not mention the unit of fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07392263])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.laplace(0, 1.5/1.3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grad(temp, model):\n",
    "    for net1,net2 in zip(model.named_parameters(),temp.named_parameters()):\n",
    "        net2[1].grad = net1[1].grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_max (loss_list, p_nmax, clip_threshold):\n",
    "    neg_loss_array = np.array([-x for x in loss_list])\n",
    "    noise = np.random.laplace(0, clip_threshold/p_nmax, len(neg_loss_array))\n",
    "    noisy_loss = neg_loss_array + noise\n",
    "    best_loss_index = np.argmax(noisy_loss)\n",
    "    return best_loss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_grad_noise(model, noise):\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        param.grad.add_(noise[i])\n",
    "\n",
    "def sub_grad_noise(model, noise):\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        param.grad.sub_(noise[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grad_Gaussian_noise(model, device, p_ng, clip_threshold, batch_size):\n",
    "    noise = []\n",
    "    # remembe that torch.normal(mean, std) use std \n",
    "    for param in model.parameters():\n",
    "        noise.append(torch.normal(0, clip_threshold/math.sqrt(2 * p_ng), param.grad.size(), device=device)/batch_size)\n",
    "    return noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "r = np.random.randint(920)\n",
    "sampler = SubsetRandomSampler(list(range(r*args.batch_size, (r+5)*args.batch_size)))\n",
    "    \n",
    "step_size_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    sampler=sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory = True\n",
    "    )\n",
    "for i,j in step_size_loader:\n",
    "    o = model(i)\n",
    "    loss = F.nll_loss(o, j)\n",
    "    loss.backward()\n",
    "noise = create_noise(model, device, args.epsilon, args.clip_threshold, args.batch_size)\n",
    "print(noise[0])\n",
    "add_grad_noise(model, noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_step_size_model(args, model, device, train_loader, p_ng):\n",
    "    \n",
    "    r = np.random.randint(920)\n",
    "    sampler = SubsetRandomSampler(list(range(r*args.batch_size, (r+5)*args.batch_size)))\n",
    "    \n",
    "    step_size_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    sampler=sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory = True\n",
    "    )\n",
    "    \n",
    "    best_loss = math.inf\n",
    "    best_lr = 0\n",
    "    best_model = Net().to(device)\n",
    "    \n",
    "    \n",
    "    if not args.best_lr_list:\n",
    "        args.alpha_max = min(args.alpha_max, 0.1)\n",
    "    elif len(args.best_lr_list) % 10 == 0:\n",
    "        args.alpha_max = (1+args.gamma) * max(args.best_lr_list)\n",
    "        del args.best_lr_list[:]\n",
    "\n",
    "    #while lr_index == 0, means choose the noise add on gradient again.    \n",
    "\n",
    "    noise = create_grad_Gaussian_noise(model, device, p_ng, args.clip_threshold, args.batch_size)\n",
    "    index = 0\n",
    "    args.epsilon -= p_ng\n",
    "    if args.epsilon < 0:\n",
    "        return model, p_ng\n",
    "    \n",
    "    while index == 0:\n",
    "        temp_loss_list = []\n",
    "        temp_model_list = []\n",
    "        temp_lr_list = []\n",
    "        add_grad_noise(model, noise)\n",
    "        \n",
    "        for i in np.linspace(0, args.alpha_max, 21):\n",
    "            temp = Net().to(device)\n",
    "            temp_loss = 0\n",
    "            temp.load_state_dict(model.state_dict())\n",
    "            #load_state_dict will not copy the grad, so you need to copy it here.\n",
    "            load_grad(temp, model)\n",
    "            \n",
    "\n",
    "            temp_optimizer = optim.SGD(temp.parameters(), lr=i)\n",
    "            temp_optimizer.step()\n",
    "            #optimizer will be new every time, so if you have state in optimizer, it will need load state from the old optimzer.\n",
    "\n",
    "            for (data, target) in step_size_loader:\n",
    "                data,target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                temp_loss += F.nll_loss(output, target).item()\n",
    "\n",
    "            temp_loss_list.append(temp_loss)\n",
    "            temp_model_list.append(temp)\n",
    "            temp_lr_list.append(i)\n",
    "        \n",
    "        #choose the best lr with noisy max\n",
    "        index = noisy_max(temp_loss_list, math.sqrt(2*p_nmax), args.clip_threshold)\n",
    "        args.epsilon -= p_nmax\n",
    "        if args.epsilon < 0:\n",
    "            return model, p_ng\n",
    "        \n",
    "        # if index == 0, means we need to add the noise again and cost more epsilon\n",
    "        if index == 0:\n",
    "            #delete the original noise and add new noise\n",
    "            sub_grad_noise(model, noise)\n",
    "            # create new noise, and also sub the epsilon of new noise\n",
    "            p_ng = (1+args.gamma) * p_ng\n",
    "            noise = create_grad_Gaussian_noise(model, device, p_ng, args.clip_threshold, args.batch_size)\n",
    "            args.epsilon -= (args.gamma * p_ng)\n",
    "            if args.epsilon < 0:\n",
    "                break\n",
    "        else :\n",
    "            best_model.load_state_dict(temp_model_list[index].state_dict())\n",
    "            best_loss = temp_loss_list[index]\n",
    "            best_lr = temp_lr_list[index]\n",
    "            \n",
    "    args.best_lr_list.append(best_lr)\n",
    "#     print(\"best learning rate:\", best_lr)\n",
    "#     print(\"best loss:\", best_loss)\n",
    "\n",
    "\n",
    "    return best_model, p_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, device, model, train_loader, epoch, p_ng):\n",
    "    \n",
    "    #init the p_nmax and p_ng\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Chose the best step size(learning rate)\n",
    "        batch_best_model, p_ng = best_step_size_model(args, model, device, train_loader, p_ng)\n",
    "        if args.epsilon < 0:\n",
    "            break\n",
    "        \n",
    "        model.load_state_dict(batch_best_model.state_dict())\n",
    "        model.zero_grad()\n",
    "        #remember to zero_grad or the grad will accumlate and the model will explode\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\talpha_max: {:.6f}\\tepsilon: {:.2f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader.dataset) ,\n",
    "                100. * batch_idx * args.batch_size / len(train_loader.dataset), loss.item(), args.alpha_max, args.epsilon))\n",
    "    return model, p_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, device, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)*(args.batch_size)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\tepsilon: {:.2f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / (len(test_loader.dataset)), args.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314098\talpha_max: 0.100000\tepsilon: 7.99\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.582042\talpha_max: 0.090000\tepsilon: 7.85\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.753495\talpha_max: 0.076500\tepsilon: 7.72\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.682755\talpha_max: 0.076500\tepsilon: 7.59\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.354116\talpha_max: 0.076500\tepsilon: 7.45\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.463463\talpha_max: 0.068850\tepsilon: 7.32\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.309969\talpha_max: 0.068850\tepsilon: 7.19\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.345216\talpha_max: 0.068850\tepsilon: 7.05\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.871764\talpha_max: 0.068850\tepsilon: 6.92\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.227886\talpha_max: 0.068850\tepsilon: 6.79\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.208587\talpha_max: 0.061965\tepsilon: 6.65\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.231822\talpha_max: 0.061965\tepsilon: 6.52\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.153419\talpha_max: 0.055768\tepsilon: 6.39\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.138253\talpha_max: 0.055768\tepsilon: 6.24\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.210166\talpha_max: 0.050192\tepsilon: 6.09\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.224992\talpha_max: 0.050192\tepsilon: 5.93\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.254091\talpha_max: 0.037644\tepsilon: 5.78\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.142927\talpha_max: 0.037644\tepsilon: 5.62\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.456748\talpha_max: 0.035762\tepsilon: 5.47\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.193423\talpha_max: 0.035762\tepsilon: 5.30\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.133913\talpha_max: 0.035762\tepsilon: 5.11\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.121119\talpha_max: 0.030397\tepsilon: 4.92\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.175788\talpha_max: 0.025838\tepsilon: 4.73\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.327585\talpha_max: 0.024546\tepsilon: 4.53\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.116352\talpha_max: 0.024546\tepsilon: 4.30\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.337789\talpha_max: 0.022091\tepsilon: 4.08\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.206143\talpha_max: 0.020987\tepsilon: 3.84\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.055972\talpha_max: 0.020987\tepsilon: 3.54\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.130073\talpha_max: 0.018888\tepsilon: 3.24\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.267890\talpha_max: 0.018888\tepsilon: 2.91\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.255815\talpha_max: 0.017944\tepsilon: 2.60\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.114847\talpha_max: 0.016149\tepsilon: 2.28\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.085878\talpha_max: 0.012919\tepsilon: 1.96\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.175546\talpha_max: 0.012273\tepsilon: 1.64\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.035290\talpha_max: 0.009205\tepsilon: 1.27\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.101292\talpha_max: 0.008745\tepsilon: 0.89\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.168107\talpha_max: 0.008308\tepsilon: 0.52\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.317969\talpha_max: 0.005400\tepsilon: 0.15\n",
      "\n",
      "Test set: Average loss: 0.1244, Accuracy: 9623/10000 (96%)\tepsilon: -0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "model = Net().to(device)\n",
    "args.best_lr_list = []\n",
    "\n",
    "args.alpha_max = 0.1\n",
    "args.epsilon = 8\n",
    "p_ng, p_nmax = args.epsilon / (2 * args.split), args.epsilon / (2 * args.split) # 30 is split number, change if we need to\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "while args.epsilon > 0:\n",
    "    epoch += 1\n",
    "    epoch_best_model, p_ng = train(args, device, model, train_loader , epoch, p_ng)\n",
    "    model.load_state_dict(epoch_best_model.state_dict())\n",
    "    test(args, device, model, test_loader)\n",
    "print(\"Spend time:{:.1f}\".format(time.time() - start))\n",
    "    \n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
